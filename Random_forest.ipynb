{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cf9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1678463987972_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-40-222.ap-south-1.compute.internal:20888/proxy/application_1678463987972_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-35-137.ap-south-1.compute.internal:8042/node/containerlogs/container_1678463987972_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Creating spark session\n",
    "spark = SparkSession.builder.appName(\"TrainRandomForest\")\\\n",
    ".getOrCreate()\n",
    "# Creating object of spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fd89a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def preprocess(df):\n",
    "# getting the numerical columns\n",
    "    cat_columns = []\n",
    "    num_columns = ['amnt','hour','days_before','weekofyear','hour_diff']\n",
    "    for column in df.columns:\n",
    "        if column not in num_columns:\n",
    "            cat_columns.append(column)\n",
    "\n",
    "    cat_columns.remove('app_id')\n",
    "    cat_columns.remove('transaction_number')\n",
    "    cat_columns.remove('__index_level_0__')\n",
    "    # creating a new dataframe\n",
    "    new_df = df.select('app_id').distinct()\n",
    "\n",
    "    # handling the categorical columns\n",
    "    for column in cat_columns:\n",
    "        # grouping by app_id and categorical column & counting the occurences \n",
    "        cnts = df.groupby('app_id',column).count()\n",
    "        \n",
    "        # extracting the maximum count and its app_id \n",
    "        max_cnt = cnts.groupby('app_id').agg(max('count').alias('count'))\n",
    "        \n",
    "        # extracting the frequently occuring value\n",
    "        mode = cnts.join(max_cnt,['app_id','count'],'inner')\\\n",
    "        .select('app_id',column)\\\n",
    "        .withColumnRenamed(column,'mode_'+column)\n",
    "        \n",
    "        # appending the newly created column to the original dataframe\n",
    "        new_df = new_df.join(mode,'app_id','inner')\n",
    "        \n",
    "    # handling numerical columns\n",
    "    for column in num_columns:\n",
    "        # calculating the average of the numerical features\n",
    "        avgs = df.groupby('app_id').agg(mean(column).alias('avg_'+column))\n",
    "        \n",
    "        # appending the column to the dataframe\n",
    "        new_df = new_df.join(avgs,'app_id','inner')\n",
    "    # returning the preprocessed dataframe\n",
    "    return new_df\n",
    "\n",
    "def getData(parquet_path,csv_path):\n",
    "    # reading the parquet file\n",
    "    df = spark.read.parquet(parquet_path)\n",
    "    # applying preprocessig to reduce the data\n",
    "    df = preprocess(df)\n",
    "    # reading the csv file\n",
    "    csv = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"True\")\\\n",
    "    .option(\"inferSchema\",\"True\")\\\n",
    "    .load(csv_path)\n",
    "    # merging the csv and the parquet file\n",
    "    df = df.join(csv,'app_id',\"inner\")\n",
    "    # droping the app_id column\n",
    "    df = df.drop('app_id')\n",
    "    # returning the cleaned data frame\n",
    "    return df\n",
    "\n",
    "def buildPipeline(df):\n",
    "    \"\"\"\n",
    "    This model creates a pipeline and return the pipeline model which can be trained\n",
    "    \"\"\"\n",
    "    # getting the name of the feature columns\n",
    "    features = df.columns[:len(df.columns)-1]\n",
    "    # creating object of VectorAssembler\n",
    "    assemble = VectorAssembler(inputCols=features,outputCol='features')\n",
    "    # creating object of classifier model\n",
    "    classifier = RandomForestClassifier(seed=7,labelCol=\"flag\",featuresCol=\"features\")\n",
    "    STAGES = [assemble,classifier]\n",
    "\n",
    "    # creating an object of Pipeline\n",
    "    pipeline = Pipeline(stages=STAGES)\n",
    "\n",
    "    # Setting the parameter grid for parameter tuning\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.maxDepth, [3,5]) \\\n",
    "    .addGrid(classifier.numTrees, [100,200]) \\\n",
    "    .build()\n",
    "\n",
    "    tvs = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(labelCol=\"flag\", predictionCol=\"prediction\"),\n",
    "    parallelism=2,\n",
    "    trainRatio=0.7)  # data is separated by 70% and 30%, in which the former is used for training and the latter for evaluation\n",
    "    return tvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "141d3863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parquet_path = \"s3://transaction-train/*.parquet\"\n",
    "csv_path = \"s3://transaction-train-etl/train_csv/alfabattle2_train_target.csv\"\n",
    "df = getData(parquet_path,csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9912aac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split = df.randomSplit([0.7,0.3],seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205056d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tvs = buildPipeline(split[0])\n",
    "model = tvs.fit(split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa721e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.bestModel.save('s3://transaction-train-etl/train_model/model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98502b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9594324003218819, 0.9594324003218819, 0.9594324003218819, 0.9594324003218819]"
     ]
    }
   ],
   "source": [
    "# f1-scores of the models\n",
    "model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16051e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create both evaluators\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"flag\", predictionCol=\"prediction\")\n",
    "\n",
    "# Make predicitons\n",
    "pred = model.transform(split[1]).select(\"flag\", \"prediction\")\n",
    "\n",
    "# Get metrics\n",
    "acc = evaluator.evaluate(pred, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(pred, {evaluator.metricName: \"f1\"})\n",
    "weightedPrecision = evaluator.evaluate(pred, {evaluator.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluator.evaluate(pred, {evaluator.metricName: \"weightedRecall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bf850b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9731265422745573\n",
      "F1  :  0.9598728180784522\n",
      "Precision : 0.9469752672792359\n",
      "Recall : 0.9731265422745573"
     ]
    }
   ],
   "source": [
    "print('Accuracy : ',acc)\n",
    "print('F1  : ',f1)\n",
    "print('Precision :',weightedPrecision)\n",
    "print('Recall :',weightedRecall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
